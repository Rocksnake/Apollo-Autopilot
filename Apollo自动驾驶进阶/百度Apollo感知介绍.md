# 感知概貌

> 感知是机器人学科的问题，因为想要制造一个机器人，必须能和环境进行感知，去处理环境的各种障碍物和突发情况。
>
> 感知的目的是寻找机器的特长并发挥出来，向人类学习并做得比人类更好。

人类开车主要是靠耳朵、眼睛、辅助例如倒车影像和雷达来获取信息。机器感知系统则是利用Lidar、Camera、Radar、超声波雷达和拾音器等构成。而且其精度比人的获取信息更高精度。

**在地图定位方面**，人类驾驶员主要依靠导航地图，或者依据交通指示牌进行路径规划和导航。机器感知系统则依赖高精地图。

高精地图和导航地图的最大差别是有参考线，汽车需要根据参考线进行行驶，而人很大程度上是根据经验驾驶。例如在十字路口，人类通常是根据经验以及前车经过十字路口或者转弯变道，而无人车是根据高精地图的参考线行驶。

输出信息的比较，人类驾驶员的感知信息输出主要是大脑输出，一是道路信息，包括车道线位置，是左转还是右转车道，是否限速，实线还是虚线等。第二是动态障碍物信息，主要包括障碍物位置、类别、形状大小、相对速度、行为意图等。除此之外，人还需要通过眼睛感知交通信号灯的状态。

无人车感知的信息输出主要是各个传感器的数据，固定目标更多的是依赖高精地图，减少在线识别。

道路信息通常以高精地图作为参考。障碍物信息通常使用障碍物检测技术获取，包括检测障碍物的位置（3D位置）、类别、形状以及障碍物跟踪。此外还需要根据不同的传感器数据进行传感器融合获得更精确的感知，保证安全驾驶。

当前无人车感知主要是通过摄像头来获取交通信号灯的颜色，灯的语义信息可以由地图提前标注好，长远的目标是通过V2X实现交通信号灯的感知。

相对人类驾驶而言，无人车系统为机器做了很多工作，把固定信息都嵌入到地图里面，在线识别只针对最小集进行处理，以保证系统的高效和鲁棒性。

**多维度剖析感知模块**，感知模块可以从小感知和大感知两个维度进行划分。小感知主要是完成感知任务需要具备的功能模块，包括检测、分割、识别、跟踪、融合等技术。大感知是指感知要真正的为无人车系统服务，需要考虑上下游，例如标定，定位，障碍物行为预测等，也是当前的一种研发趋势。

感知模块的维度可以根据划分目标而不同：

1. 传感器维度，称之为输入
   * 主要有Lidar，Camera，Radar，Ultrasonic高精地图等。
2. 输出，包括障碍物，车道线和道路边界，红绿灯等。
3. 问题空间维度，按照时间可以分为静态帧检测和时序处理等。
4. 机器视觉维度，分为高语义和低语义问题，例如模型计算和几何计算。
5. 机器学习维度，分为深度学习驱动的感知和通过先验知识进行启发式设计的方法，称之为后处理。
6. 系统维度，包括硬件和软件，以及软硬件一体化。在无人车系统里，感知的硬件显得更为重要，软件需要同步于硬件。

# 传感器选择和安装

> 讲述传感器的选择和标定

## 激光雷达

测距原理：激光头发射光束，达到障碍物就反射回来，计算时间测得距离，这种测距方式也叫做TOF（飞行时间测距法）。

激光雷达传感器好处是自带光源，不受环境干扰，缺点是多线扫描还比较稀疏，特别是远的时候。

64线激光雷达的感知距离只有60-70米，对于高速行驶的无人车还不够。

## 相机

> 被动式的

相机是最像人的一个传感器。它的优点是可以稠密感知，感知所有细节。缺点是单目相机测距不准。

## Radar毫米波

毫米波雷达的原理和激光类似，只不过发射的是毫米波。由于它也是**主动式**感知设备，不太受天气、光照的影响。同时毫米波雷达还有多普勒频移效应可以测量与障碍物之间的相对速度。

其优点是测距、测速比较准，缺点是噪点很多，例如在空旷的地方反馈很多的回波，实际可能是路面的反馈信号而不是障碍物。其次它对于非金属的反射信号比较弱，召回比较低，例如在它面前走过行人有可能漏掉。最后毫米波雷达也是**稀疏感知，无法做识别任务**。

除了以上三种常见的传感器，还有一些用的相对少的传感方法，例如超声波，高精地图，Image-Lidar。

## 选传感器

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200815225344876.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70#pic_center)

## 安装传感器

> 传感器的安装还需要从整车的角度考虑，需要考虑安全，风阻，美观和清洗等因素

1. 障碍物遮挡情况，这就是为什么很多无人驾驶车的传感器安装在车顶的原因。
2. 方便传感器融合。传感器的融合需要不同传感器在视野上有重叠，否则很难进行传感器融合。

# 传感器标定

## 标定

> 得到传感器之间的相对位置，将不同传感器的数据在同一个坐标系中表示。

* 内参标定
  * 内参是传感器自身性质，有些是厂家提供，有些需要自行标注，例如Camera焦距的定期矫正，Lidar中各激光管的垂直朝向角。
* 外参标定
  * 外参是传感器之间的相对位置和朝向，一般由6个自由度表示，自由度的旋转矩阵和自由度的水平位移。

## 标定算法

* Lidar内参标定：相对于摄像头，激光雷达的内参标定有一定的特殊性。
  * 借助Rigel，在开阔平坦的场景下进行完整扫描，得到场景的点云。然后，将要标定的激光雷达对同一个场地进行同样的扫描，之后对两次扫描的点云进行匹配，如果激光雷达的内参不准确，会出现远距离地面点浮在空中的现象。
* Lidar-to-GPS外参标定：计算出Lidar与车上GPS的相对位置。GPS的位置由世界坐标系中的3D点表示，因此计算出外参之后可以计算得到Lidar在世界坐标系中的位置。
  *  将车辆在空旷地区绕八字行驶多次，记录多个时间点GPS给出车辆的位置信息，将Lidar捕捉的多帧点云投射到世界坐标系中，并进行拼接，求解优化获得外参。
* Lidar-to-Lidar外参标定：计算不同Lidar之间的相对位置。
* Lidar-to-Camera外参标定：计算Lidar和相机之间的相对位置。
  * 将Camera看到的所有Tag角点与Rigel扫出的对应点的3D坐标进行匹配，求解PNP问题，得到Camera相对于Rigel世界坐标系的相对位置。
  * 将Lidar激光雷达扫描得到的点云与Rigel是稠密的点云之间进行ICP（Iterative Closest Point，迭代最近点）匹配，得到激光雷达相对于Rigel的位置。最后将两个位置矩阵相互传递得到Camera 和 Lidar之间的相对位置，具体流程如下所示。其标定结果验证方法是将激光雷达感知到的3D点云投影到Camera拍摄的图像上，看边界是否一致，如果不一致需要重新标定。
* Camera-to-Camera外参标定：计算不同相机之间的相对位置。
* 自然场景中的Lidar-to-Camera外参标定：在自然环境中，驾驶车辆进行两种不同传感器之间的位置关系求解。
  * 由于没有了Rigel-Tag，需要在标定的场景中寻找具有明显边缘的物体作为参照物，将该参照物的点云边缘和Camera拍摄图像的边缘对齐作为参数标定效果的考量指标，如果对齐，这表示标定结果良好。
* 自然场景中的Bifocal Camera外参标定：双焦点摄像头之间外参的计算，也是就是不同焦点的相对位置。
  * 过程类似于Camera-to-Camera外参标定，但是需要在自然场景中找到边缘锐利的物体作为参照物。
* Camera-to-Radar外参标定：摄像机到毫米波雷达的外参计算。
  * Camera-to-Radar外参标定的重点是得到Radar相对于Camera的Pitch角的上下倾斜问题。通常情况下Radar是水平安装的，问题就转换为求得Camera相对于地面的Pitch角。由于Camera-to-Lidar已经标定好，可以通过Lidar采集到的平面信息解决倾角问题。

## 3D 标定间制作

在空旷房间的墙面贴满不同的二维码，然后在标定间中间放置一个基于激光雷达的毫米级高精度Rigel激光扫描仪，通过对墙上二维码的多次扫描，完成3D建模，获得了标定间任何一个点的3D位置。

根据建模结果输出一个查找表，查找表由两栏组成，第一栏是Tag_ID，表示各个二维码的ID编号，第二栏是二维码四个角的3D位置信息。之后即可在标定间进行不同传感器的参数标定。

# 感知算法

> 感知算法根据使用的传感器不同而不同。对于激光雷达。本节会介绍两个检测算法，分别是启发式的Ncut和深度学习算法CNNSeg。

## 点云感知

> 感知障碍物的位置、大小、类别、朝向、轨迹、速度等。核心是点云检测分割技术，可以用启发式算法NCut和深度学习算法CNNSeg完成。

**Ncut算法**的基本思想是基于空间平滑性假设，即空间上接近的点来自同一个障碍物。

首先，利用地图信息对点云进行预处理，例如去掉感兴趣区域之外的点云，降低点云图的复杂度。然后根据预处理后的点云构建加权图G=(V, E, W)，将点云分割转换为图分割的问题，可以利用图聚类的算法求解，最终求解的每一个cluster就代表一个障碍物。

该算法的优点是解释性好，缺点是分割规则过于简单，难以应对实际情况（草丛、绿化带），主要原因在于缺乏语义信息。

**深度学习方法：CNNSeg**利用卷积神经网络来处理激光雷达捕获的点云数据，并对点云中的目标进行识别。

深度学习是数据驱动的，它把人工构造特征这一任务交给机器和算法去完成（即特征学习）。

**研发历程：**

1. 将所有点云都投到前视图（front-view）（投影面是一个圆柱面）来构造特征，将点云问题转化为矩阵问题，进而使用深度学习进行处理。通过构建全卷积神经网络对前向视图进行处理。
2. 借助自采集车队，采集更多的实际数据，并且扩展数据视角，制作俯视图，通过将俯视图+前视图相结合的方式进行训练。同时，修改Loss函数，包括使用3D回归和Segmentation的损失函数。经过多次实验，发现基于俯视图的Segmentation方法效果最好。该方法目前已经在Apollo中开源.
3. 因为俯视图没有高度信息，因此我们把前视图和Camara图像加进来进行辅助检查，利用Lidar的测距准和Camera识别准优势完成了Middle-Level Fusion方法（Multi-View 3D Object Detection Network for Autonomous Driving），发表在CVPR 2017。该方法使用俯视图提取Proposal，利用前视图和光学图像辅助进行更加精准的位置回归。

## 视觉感知

> 最早从ADAS发展而来。ADAS算法相对轻量，采用人工构造的特征和浅层分类器的方式实现辅助驾驶，该方法目前已经难满足自动驾驶的需求。

随着深度学习技术的发展，尤其是在视觉领域的巨大成功，视觉感知的主流技术路线已经变为“深度学习+后处理计算”的方法。

变化：① 要求计算硬件升级；② 数据的需求量大增；③ 如何评估保证安全性

**面向自动驾驶的深度学习算法的特点：**

1. 2D感知向3D感知渗透，模型输出更丰富（后处理需要的3D信息、跟踪信息、属性信息等都会放在CNN中进行学习）
2. 环视能力构建（传统方法靠一个Camera完成前向检测、碰撞检测、车道线检测。无人驾驶需要环视）
3. 感知+定位+地图紧密结合

CNN检测是深度学习里一个十分火热的应用，从最开始的AlexNet,VggNet到ResNet等，持续提高了ImageNet图像检测的精度，更多的细节和最新进展可以参考Kaiming He（何凯明）的系列工作。需要注意的是，目前发表的大部分关于检测的成果都是面向计算机视觉应用的，与自动驾驶领域的检测还有很大的区别。

1. 自动驾驶中，摄像头是安装在车上的，汽车行驶在结构化、规则化道路上，面向的场景更具体，有很多的几何约束可以用来辅助检测。

2. 自动驾驶中的检测模型需要输出的信息更多，包括障碍物的尺寸、朝向。同时自动驾驶还需要考虑时序性。我们称之为局部的End-to-End，检测、2D到3D转换、跟踪三步是自动驾驶视觉感知的组成，后面两步都由CNN来学习，减少人工干预。

3. 多任务学习，网络结构适配。自动驾驶需要针对不同的障碍物特征（车道线，道路边界，定位元素）进行识别，如果分别由专用模型处理，整个处理流程太长，无法满足要求，因此需要做多任务的识别和网络结构的适配。

4. 属性识别，即除了障碍物级别的输出以外，还需了解速度类别朝向等问题，例如车尾灯状态，车门开闭状态。可以通过标注数据，由数据驱动的深度学习来自动学习这些更细的属性。

## CNN分割

分割（Segmentation）与detection在本质上是一样的，是对一个目标的不同力度的刻画。

分割是一种更细粒度刻画物体边界信息的检测方法，不再是画框，而是进行边缘分割。

在感知中，对于不规则物体，往往需要进行分割处理。例如场景分割和可行驶区域感知。

场景分割可以用于多传感器融合，例如对绿植进行分割，结合LiDAR点云，就可以测出绿植的类别和距离。

可行驶区域则可以对一些非结构化道路的路径规划提供支持。在车道线感知中，应视具体情况使用分割或者检测方法。

## 后处理

一个完整的系统除了深度学习模型，还需要做一些后处理，后处理是直接针对下游模块，对后续的影响比较直接。

在视觉感知中，后处理包括：

1. 2D-to-3D的几何计算，需要考虑：① 相机pose的影响；② 接地点；③ 稳定性
2. 时序信息计算，针对跟踪处理，需要注意：① 对相机帧率和延时有要求【要求轻量级】；② 充分利用检测模型的输出信息（特征、类别）进行跟踪；③ 考虑轻量级Metric Learning
3. 多相机的环视融合，相机布局决定融合策略，要做好视野重叠 

## 红绿灯感知

>  红绿灯感知是百度无人车第一个使用深度学习、使用Camera的模块。

在当时使用深度学习、GPU和Camera做红绿灯感知是存在很多争议的，但是我们一直坚持走这种探索性的道路，并且成功了。现在车上用深度学习、GPU、Camera进行感知已经是一种共识。

**红绿灯感知的任务是在距离停止线50～-2米的范围内精准识别红绿灯亮灯状态**。它的**难点**在于检测精度要求非常高，必须达到三个九（99.9%），否则会出现闯红灯，违法交规的情况。另外召回也不能太低，假设一直是绿灯，但是一秒只召回一帧绿灯，其他都认为是识别错的红灯，则通不过路口，影响通过率和体验。第二是红绿灯感知需要应对各种环境，包括天气和光照。最后是红绿灯的制式非常多，包括距离、高度、横纵、信号状态等，红绿灯感知都需要识别。

**基于深度学习的红绿灯感知模块**，其构建分为以下几步：

1. 相机选择和安装。相机选择需要注意几个重要的参数，一是相机的高动态比HDR要高于100db。第二是拍摄图像的分辨率要达到1080P，满足15pixel大小的灯能够看得到。在实际应用中，我们使用了两个Camera，叫双Camera，长短焦切换，六毫米适用于近距离。

2. 高精地图的交互。我们并不是把所有红绿灯识别算法都交给在线算法完成，这样做无法满足鲁棒性要求也加重算法负担，需要把一些固定信息交给高精地图去完成，算法只实现在线变化的一些因素。地图提供灯组3D坐标和交通含义，算法只看RoI区域，避免形状识别出错。

3. 使用深度学习识别灯颜色的变化。主要分为检测和颜色分类两步。

4. 实际使用过程中可能会更复杂。如上图所示，一条道路上有很多红绿灯，检测算法会把所有的灯都检测出来，地图会告知需要看几个灯，但是并不知道看哪几个灯。因此需要把对应关系匹配起来，需要做3D到2D的投影，投影又受到标定、定位、同步、地图等因素的影响，需要综合考虑才能解决好这个问题。

## Radar感知

通过一个实际的点云信息投影的成像，对图进行分析：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200815231747216.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70#pic_center)

图中有一条明显的道路边界线，这是道路两边的特质栏杆反射信号好，才会使雷达点成直线发布。

## 超声波感知

> 超声波只能进行近距离感知，并且无法感知具体的位置

对无人驾驶来说，行驶过程中帮助并不大，更多的是用于倒车和特别近距离的感知。

当某个地方有障碍物，传感器发送超声波，收到回波信号以后可以得到障碍物的距离信息，但是并不能确定在覆盖范围内哪个地方有障碍物。

# 机器学习

> 机器学习里面存在一个普遍的假设，训练集和测试集是独立同分布的，如果测试和训练没有任何关系，测试效果是没有任何保证的。但是无人车感知的训练集是封闭的，而测试集是开放的。

测试是在开放道路进行的，如果遇到新的障碍物，在训练中从来没见过，怎么处理？

E.g.：在城市道路上很少见到卡车，但是高速会遇到很多卡车，而且卡车上的东西也很多，如果机器学习模型没见过这些障碍物，很有可能带来一些错误。

## 可解释性是否需要

无人车的安全需要可解释，出现一个Bad case需要说清责任，需要搞清Bad case是由什么原因导致的，以便改进。

自动驾驶中深度学习模型需要更好的可解释性，归结为如何评估模型让用户知道模型是安全的？模型更新后如何做回归测试？模型的应用边界在哪里？目前，大家认为可解释性可以通过测试来体现，如果大量测试得到相同的结果，那么原理是不是真正的可解释也就没那么重要。

## 其他

在感知模块中，除了做基本的检测、分割之外，还有后处理阶段等由公式表示的几何计算问题，是不需要深度学习的。

另外，Common sense也不需要深度学习，而且深度学习的效果不好，我们需要其他算法。那么，深度学习模型带来一个结果，其他启发算法给一个结果，怎么来融合？现在主要是基于Double check来提升安全性，还需要其他方法来进行更好的融合。

除了深度学习还需要其他的机器学习方法。如果数据量小，特征很难从原始数据学习，深度学习的效果可能就受到影响，因此诸如SVM或者随机森林这些机器学习算法，可能需要结合场景选择。

# 感知的未来

## Sensor 迭代

如果在自动驾驶的研究中，发现某一类传感器在感知或者其他模块中具有很大的价值。

那么，整个资本市场会投入很多人力、财力研发传感器。随着量产之后，传感器的成本就会大幅下降，更新换代就比较快。

## 深度学习 + 仿真数据 + AI 芯片

深度学习已经证明了在感知中有很大的作用，但是计算量很大，专门研究车载AI芯片是对这一问题的很好解决方案。

现在很耗时的CNN模型以后都不是瓶颈，而且定制AI芯片的功耗可以足够低，满足车载需求。

深度学习需要大量数据的问题，可以通过仿真来弥补。目前，点云仿真相对简单一些，图像仿真相对困难点。如果仿真这条路可以走通，那么仿真+深度学习不断循环迭代，是非常有前景的。

## 智能交通设施

目前，自动驾驶都是在车上安装传感器进行感知，感知范围、鲁棒性都有待提高，如果将这套传感器步置在道路上、灯上，让它们来感知，然后将实时结果传输给无人车。

如车上的传感器失灵，那么路面上的传感器会告知无人车障碍物信息，保证系统安全性。另外在驾驶环境中部署传感器可以拓展感知距离，做到足够安全，提前告知远处的信息。

# 课后思考

**Q:**

延时是如何来影响自动驾驶安全的？

**A:**

面对突发情况时，即便是很微小的延时都有可能导致不可估量的后果。不管是反应延时、传播延时、处理延时，都应最大程度地缩小



**Q:**

ACC自适应巡航的功能是如何实现的？ACC这些功能主要是用哪些传感器来实现的？原理是什么？

**A:**

ACC自适应巡航控制系统是一种基于传感器识别技术而诞生的智能巡航控制，相比只能根据驾驶者设置的速度进行恒定速度巡航的传统巡航控制系统，ACC可以对于前方车辆进行识别，从而实现了“前车慢我就慢，前车快我就快”的智能跟车的效果，目前根据使用速度区段，可分为基本版ACC（30-150km/h）和全速ACC（0-150km/h）。

其中，基本版ACC的传感器为雷达，而全速ACC则是在雷达为主要传感器的前提下，加入了前视摄像头等其他传感器的辅助识别，以满足低速时对于识别精度和角度的更高要求。

**毫米波雷达原理**：利用目标对电磁波反射来发现目标并测定其位置，毫米波频率高，波长段。

**单脉冲雷达原理**：雷达每发射一个脉冲，天线能同时形成若干个波束，从各波束接收的信号之和，可测出目标的距离，从而实现对目标的测量和跟踪。（脉冲：一个物理量在短持续时间内突变后迅速回到其初始状态的过程）

**微波雷达原理**：微波雷达对运动物体的精确速度检测基于微波多普勒（Doppler）效应。通过测量回波信号相对发射信号的时间延迟来测距。

**激光雷达原理**：激光器产生并发射一束光脉冲，打在物体上并反射回来，最终被接收器所接收。接收器准确地测量光脉冲从发射到被反射回的传播时间。因为光脉冲以光速传播，所以接收器总会在下一个脉冲发出之前收到前一个被反射回的脉冲。鉴于光速是已知的，传播时间即可被转换为对距离的测量。

**红外探测雷达原理**：不同种类的物体发射出的红外光波段是有其特定波段的，人们可以利用这种特定波段的红外光来实现对物体目标的探测与跟踪。